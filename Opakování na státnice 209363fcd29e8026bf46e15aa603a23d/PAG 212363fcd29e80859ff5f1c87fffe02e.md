# PAG

# Terminilogy

**NUMA** - non-uniform memory access

**UMA** - uniform memory access

**PRAM** - Parallel Random Access Machine, jeden procesor

**RAM** - Random Access Machine více procesorů, sdílený globální paměťový prostor

# Communication models

**Static network** - hvězdice, kružnice, …

**Dynamic network** - sběrnice

## Network Topolgies

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image.png)

Linear Array, ring

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%201.png)

Mesh

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%202.png)

Hypercube

## Communication Operations

### One-to-All Broadcast & All-to-One Reduction

Cíl této operace je z jednoho $p$ rozeslat informaci do všech nebo reverzně ze všech do jednoho $p$. 

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%203.png)

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%204.png)

Podobný princip i na ostatních typech sítí. Vždy se snažíme z odesílání udělat strom.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%205.png)

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%206.png)

Procházím sítí v podstatě jako binárním stromem, takže složitost vždy bude $\log_2p$ a celově časová složitost operace bude rovna $T = (t_s + t_wm)\log p$, kde 

- $t_s$ je čas potřebný k nastartování node,
- $t_wm$ je doba odeslání zprávy velikosti $m$.

### All-to-All Broadcast and Reduction

Ze všech $p$ chci odeslat informaci do všech $p$. Nejjednodužší přístup je zopakovat $p$ operaci *One-to-All Broadcast* či *All-to-One Reduction*. To by ale nevedlo k vysoké efektivitě.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%207.png)

Místo toho je na Ring jedodužší provést $p-1$ posun všech známých operací přidání nových, které jsou na daném node.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%208.png)

Na mesh provést shift po dimenzích a analogicky i na hypercube, tedy také po jednotlivých dimenzích, tedy každý node přenese data mezi sebou a všemi svými sousedy.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%209.png)

Operace bude náročná na:

- ring $T = (t_s + t_wm)(p-1)$
- mesh $T = 2t_s(\sqrt p - 1) + t_wm (p - 1)$
- hypercube $T = \sum^{\log p}_{i=1}(t_s + 2^{i-1}t_wm)$

### All-Reduce and Prefix-Sum Operations

Rozešli vektor $m$ do všech node a proveď na nimi nějakou operaci, která bude ve všech $p$ stejná. Konkrétně by šlo operaci rozložit do kroků:

1. Proveď *All-to-One Reduction* s vektorem
2. Proveď výpočetní operaci nad vektorem
3. Rozešli data skrze *One-to-All Broadcast* s upraveným vektorem do všech $p$

Tento proces je ale neefektivní, proto se využívá pattern *all-to-all broadcast.* Specifickým příkladem je pak **Prefix-sum** operace. Nikdy se ale neinkrementuje velikost vektoru.

**Úloha:** Mějme $p$ čísel $n_0, n_1, n_2, ..., n_{p-1}$ (id pro každý node). Cílem je spočítat sumu $s_k = \sum^k_{i=0}n_i$ pro každé $k$ mezi $0$ a $p-1$. Tedy spočítat sumu id pro každý $p$ a to jako sumu všech předchozích *id.*

**Řešení:** proveď výpočet jako binární strom.

Časová složitost takové operace je rovna: $T= (t_s + t_mw)\log p$

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2010.png)

### Scatter and Gather

**Scatter** je taková operace, která odešle specifickou zprávu o velikosti $m$ všem ostatním nodům (*one-to-all personalized communication*).

Proces je totožný jako při O*ne-to-All broadcast* operaci, pouze odesílá customizovanou zprávu.

**Gather** je operace, kdy jeden node přijme unikátní zprávu od všech ostatních nodů. A proces je totožný jako Scatter jenom reverzně provedený.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2011.png)

Cena těchto operací je rovna $T = \sum^{\log p}_{i=1}(t_s + t_w p\frac{m}{2^i})$ 

### All-to-All Personalized Communication

Odesílá z každého *node* personalizovanou zprávu do každého jiného *node*. Tomuto spojení se říká **total exchange** protože je třeba odeslat zprávu mezi každým nodem.

Na hypercube využívá `i XOR j` operace.

Myšlenka je taková, že přidej dimenzi a pak projeď skrze všechny node s využití `XOR` operace.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2012.png)

### Circular Shift

Proveď posun dat o $q$ node, tedy node $i$ odešle data do node $(i + q) \text{ mod }  p$, kde musí platit $(0 \leq q \leq p)$.

Na ring je tato operace intuitivní.

Provedeme počet shift operací ve směru a následně přeneseme podle nutnosti ve směru druhém. *Někdy může být kratší posun druhým směrem.*

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2013.png)

Neexistuje žádný iniverzální algortimus, je dobré napočítat pro každý specifický posun indovodiálně.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2014.png)

| Operation | T (ring) | T (meash) | T (hypercube) |
| --- | --- | --- | --- |
| One-to-All Broadcast, All-to-One Reduction | $(t_s + t_wm)\log p$ | $(t_s + t_wm)\log p$ | $(t_s + t_wm)\log p$ |
| All-to-All Broadcast, All-to-All Redcution | $(t_s + t_wm)(p-1)$ | $2t_s(\sqrt p - 1) + t_wm (p - 1)$ | $\sum^{\log p}_{i=1}(t_s + 2^{i-1}t_wm)$ |
| All Reduce | $(t_s + t_wm)\log p$ | $(t_s + t_wm)\log p$ | $(t_s + t_wm)\log p$ |
| Scatter & Gather | $t_s\log p + t_wm (p-1)$ | $t_s\log p + t_wm (p-1)$ | $t_s\log p + t_wm (p-1)$ |
| All-to-All Personazed Communication | $\sum^{p-1}_{i=1}(t_s + t_wm(p-i))$ | $2(t_s+t_wm* \sqrt p)(\sqrt p - 1)$ | $(t_s + t_wm)(p-1)$ |
| Circular Shift | $q$ | $(t_s+t_wm)(\sqrt p + 1)$ | $(t_s + t_wm)(2\log p - 1)$ nejlépe $t_s + t_wm$ |

# Parallel metrics

**Speedup** o kolikrát nám zrychlí paralelní verze sériovou $S = \frac{T_s}{T_P}$, kde $T_s$  je sériová implementace a $T_P$ je impelmentace paralelní.

**Amdahl’s Law** je zákon, který nám říká, že kód vždy obsahuje sériové části kódu $\beta$ a ty nelze parelelizovat, tedy pro paralelní část platí $(1-\beta)$. Tedy existuje limit, který nevyřešíme přidáním dalšího procesoru.

Tedy ***speedup*** je omezený $S \leq \frac{T_S}{\beta T_S + (1-\beta)T_S/p}=\frac{p}{\beta p + (1-\beta)}$

**Efficiency**  je míra, která nám říká, zdali jsou jednotlivé procesory efektivně využity podle vzorce $E = \frac{S}{p}$. Tedy, pokud bude $E= 1$ pak jsou využívány všechny procesory, pokud bude menší než $1$, tedy $E \leq 1$, pak nejsou efektivně využívány všechny. Hodnota musí být vyšší než $0$ a menší nebo rovna $1$.

**Isoefficiency (W)** je míra, která určuje, jak moc musí růst velikost problému v závislosti na počtu procesorů $p$, aby zůstala rychlost řešení konstatní. Tedy jedná se o ideálné míru pro ilustraci škálovatelnosti.

**Cost** je hodnota, která zobrazuje celkový čas, který všechny procesory dohormady stráví na dané úloze, tedy $C= p \cdot T_p$.

**Cost optimal** je taková úloha pokud je jejíc cena $C$ asymptoticky stejná jako čas sériového algortimu $T_S$. Tedy $C=p \cdot T_p \in \Theta(T_s)$.

### Příklad paralelního sčítání čísel

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2015.png)

# Matrix multiplication

## Matrix-vector multiplication

Mějme matici $A$ s $n$ x $n$ řádky a vektor $x$ s $n$ prvky a výsledný vektor $y$. Séroiový algortimus vžaduje $n^2$ násobení a sčítání.

### 1D Partition

Každý procesor řeší jeden sloupec matice. Postup je následující:

1. Provede *All-to-All Broadcast*
2. Každý procesor spočít jeden sloupeček
3. Provede *All-to-One Reduction*.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2016.png)

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2017.png)

Využívá počet procesorů roven dimenzi matice, tedy $p= n$. Algortimus je **cost-optimal**.

### 2D Partition

Je schopen využít až $n^2$ procesorů. Algotitmus funguje s rychlostí $\Theta(\log p )$. Otázka ale zní, zdali komunikace není náročnější než samotný výpočet. Postup je následující: vynásobení provede každý $p$ sám, následné sčítání provádí po řádku. Algortimus není **cost-optimal**.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2018.png)

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2019.png)

## Matrix-matrix multiplication

Sériová verze je má složitost $\Theta(n^3)$.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2020.png)

### Cannon’s Algorithm

Paralelní verzi je možné realizovat s využití **Cannon’s Algorithm**, který využívá $\sqrt p$ procesorů. Myšlenka tohoto algoritmu je taková, že využívá rotace datových bloků mezi procesy tak, aby každý proces vždy získat portřebnou část matice.

Tento algortimus je **cost-optimal** a navíc je i **memory-optimal**.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2021.png)

### DNS algorithm

Využívá namapování matice na krychli a realizje násobení na ní.

Napapuji tak, abych mohl využít $n^3$ processorů. Každý procesor počítá samostatně sčítání-násobení.

Algoritmus není **cost-optimal**. Je možné takovou verzi udělat a to využít pouze $n / \log n$ procesorů.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2022.png)

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2023.png)

# Parallel algorithms

## Sorting networks

Základním blokem sorting networks je comparator. Má vždy 2 vstupy a dva výstupy a realizuje operaci podmíněného prohození podle toho mu říkáme **increasing compoarator** ⊕ ****nebo **decreasing comparator** ⊖.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2024.png)

### Bitonic sort

Bitonic Sort je algortimus, který je postavený na sorting network. Umí provádět seřazení $n$ prvků v s asymptotickou časovou složitostí $\Theta(\log^2n)$.

**Bitonic sequence** je množina (sekvence) $s=\langle a_0, a_1, a_2, ..., a_{n-1} \rangle$ pro kterou platí, že $a_0 \leq a_1 \leq a_2 \leq ... \leq a_{n/2}$ a $a_{n/2} \geq a_{n/2+1} \geq ... \geq a_{n-1}$.

Existuje ve dvou možnostech, podle toho, jaké řazení realizuje:

- increasing
- decreaing

Pro realizaci Bitonic sort se využívá **bitonic merging network**. Síť obsahuje $\log n$ sloupců, každý sloupec obsahuje $n/2$ komparátorůa provádi krok bitonic merge.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2025.png)

Pokud mám bitonic sequence, je proces jednoduchý. V případě ale náhodného rozdělení čísel, které nesplňuje podmínku bitonic sequence to není již tak jednoduché. Proto v praxi musíme nejpreve sestavit bitnonic sequence.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2026.png)

## Enumeration sort

Algortimus počítá, kolik prvků je menší než on a poté uloží hodnotu na daný index.

Paralení verze používá $n^2$ procesorů v čase $\Theta(1)$. Ukádá data do 2D mřížky $n \times n$, kde každý sloupec reprezentuje jednu hodnotu. Každý procesor vyhoná porvnání a přídá buď $1$ nebo $0$ pokud není hodnota větší.

Funguje na PRAM modelu, který předpokládá, že je PC schopen v jednom kroku schopen zapsat $p$ procesy do jedné paměti a data se neztratí.

## Parallel finding connected components

1. Rozdělím graf na 2 částí pro každý procesor
2. Pro každý spočítám MST
3. Procházím A a pokud není v B spojená, spojím je.
4. Výsledkem je B.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2027.png)

## Parallel maximal independent set

Používá se Luby’s algorithm.

![image.png](PAG%20212363fcd29e80859ff5f1c87fffe02e/image%2028.png)